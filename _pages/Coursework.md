---
layout: archive
title: "Coursework"
permalink: /coursework/
author_profile: true
---

## MIT Machine learning for inverse graphic ##
The following notes is my personal understanding of the inverse graphic course. I have summarised all the topics that professor Vincent Sitzmann has covered. 

**The topic which have been covered is the following:**
1. Computer vision & computer graphics fundamentals (pinhole camera model, camera pose, projective geometry, light fields, multi-view geometry).
2. Volumetric scene representations for deep learning: Neural fields & voxel grids.
3. Differentiable rendering in 3D representations and light fields.
4. Inference algorithms for deep-learning based 3D reconstruction: convolutional neural networks, auto-decoding.
5. Basics of geometric deep learning: Representation theory, groups, group actions, equivariance, equivariant neural network architectures.
6. Self-supervised learning of scene representations via 3D-aware auto-encoding.
7. Applications of neural scene representations in graphics, robotics, vision, and scientific discovery.

[Here is the pdf](https://georget27.github.io/files/Machine_learning_for_inverse_graphic.pdf)

## A Guide into Vector Space, Quotient space, Dual Space and Group Theory ##
I worked with Prof Edward Segal into area such as dual space and quotient space which directly followed up by Alegra 2 and Alegra 3 content during the summer in 2023. 

 My collaboration with Prof Edward Segal enabled me to approach machine learning from a theoretical perspective by studying linear algebra. I revisited vector space concepts, including the Finite-Dimensional Vector Space Isomorphism Theorem, and derived the transformative first isomorphic theorem on quotient space. As I transitioned into graph theory, I discerned the broader applications of the isomorphic theorem in maintaining the connectivity structure. 
 
